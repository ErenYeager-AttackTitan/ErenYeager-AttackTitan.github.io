I"}<p>Variational inference methods in Bayesian inference and machine learning are techniques which are involved in approximating intractable integrals. Most of the machine learning techniques involves finding the point estimates (Maximum likelihood estimation (MLE), Maximum a posteriori estimate (MAP)) of the latent variables, parameters which certainly do not account for any uncertainty. For taking account of uncertainty in point estimates, an entire posterior distribution over the latent variables and parameters is approximated.</p>

<p>    </p>

<p><img src="/img/posts/Gaussian/2.png" alt="" /></p>

<p>    </p>

<p>For approximation of posterior distribution, which involves intractable integrals variational inference methods are applied for making the calculation tractable (having some kind of close form mathematically !!) i.e. which does not involve calculation of marginal distribution of the observed variables P(X).</p>

<p>    </p>

<p><img src="/img/posts/Gaussian/3.png" alt="2" /></p>

<p>    </p>

<p>The marginalisation over Z to calculate P(X) is intractable, because of the search space of Z is combinatorially large, thus we seek an approximation, using variational distribution Q(Z) with it’s own variational parameters.</p>

<p>    </p>

<h2 id="variational-inference-as-an-optimisation-problem">Variational Inference as an Optimisation problem</h2>

<p>Since we are trying to approximate a true posterior distribution p(Z|X) with Q(Z), a good choice of measure for measuring the dissimilarity between the true posterior and approximated posterior is Kullback–Leibler divergence (<em>KL-divergence</em>),</p>
:ET